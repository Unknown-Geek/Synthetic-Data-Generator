{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Data Pipeline\n",
    "This notebook demonstrates the synthetic data generation pipeline, including packaging, monetizing, and marketing the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ctgan import CTGAN\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from typing import List, Dict, Optional\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "class SyntheticDataPipeline:\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_file: str,\n",
    "        categorical_columns: List[str],\n",
    "        output_dir: str = \"output\",\n",
    "        metadata: Dict = None\n",
    "    ):\n",
    "        self.input_file = input_file\n",
    "        self.categorical_columns = categorical_columns\n",
    "        self.output_dir = os.path.abspath(output_dir)\n",
    "        self.metadata = metadata or {}\n",
    "        self.logger = None\n",
    "        \n",
    "        # Ensure output directory exists and is empty\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        self._cleanup_output_directory()\n",
    "        self._setup_logging()\n",
    "\n",
    "    def _cleanup_output_directory(self):\n",
    "        \"\"\"Remove all files in the output directory except .gitkeep\"\"\"\n",
    "        # Close logging handlers before cleanup\n",
    "        if self.logger:\n",
    "            for handler in self.logger.handlers:\n",
    "                handler.close()\n",
    "            self.logger.handlers.clear()\n",
    "\n",
    "        for filename in os.listdir(self.output_dir):\n",
    "            if filename != '.gitkeep':\n",
    "                file_path = os.path.join(self.output_dir, filename)\n",
    "                try:\n",
    "                    if os.path.isfile(file_path):\n",
    "                        os.unlink(file_path)\n",
    "                    elif os.path.isdir(file_path):\n",
    "                        shutil.rmtree(file_path)\n",
    "                except Exception as e:\n",
    "                    print(f'Error deleting {file_path}: {e}')\n",
    "\n",
    "    def _setup_logging(self) -> None:\n",
    "        \"\"\"Setup logging with a temporary log file.\"\"\"\n",
    "        self.logger = logging.getLogger(\"SyntheticDataPipeline\")\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        \n",
    "        # Clear any existing handlers\n",
    "        if self.logger.handlers:\n",
    "            for handler in self.logger.handlers:\n",
    "                handler.close()\n",
    "            self.logger.handlers.clear()\n",
    "            \n",
    "        formatter = logging.Formatter(\n",
    "            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        \n",
    "        # Use a temporary directory for the log file\n",
    "        temp_dir = tempfile.gettempdir()\n",
    "        log_file = os.path.join(temp_dir, \"pipeline.log\")\n",
    "        \n",
    "        fh = logging.FileHandler(log_file)\n",
    "        fh.setFormatter(formatter)\n",
    "        self.logger.addHandler(fh)\n",
    "\n",
    "    def load_data(self) -> pd.DataFrame:\n",
    "        self.logger.info(f\"Loading data from {self.input_file}\")\n",
    "        try:\n",
    "            data = pd.read_csv(self.input_file)\n",
    "            self.logger.info(f\"Loaded {len(data)} rows of data\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading data: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def preprocess_data(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Preprocess data by properly handling types and missing values\"\"\"\n",
    "        processed_data = data.copy()\n",
    "        \n",
    "        # Only process and return user-selected categorical columns\n",
    "        selected_data = processed_data[self.categorical_columns].copy()\n",
    "        \n",
    "        # Handle categorical columns\n",
    "        for col in self.categorical_columns:\n",
    "            # Convert to string and handle missing values\n",
    "            selected_data[col] = selected_data[col].fillna('MISSING')\n",
    "            selected_data[col] = selected_data[col].astype(str)\n",
    "        \n",
    "        self.logger.info(f\"Preprocessed {len(self.categorical_columns)} categorical columns\")\n",
    "        return selected_data\n",
    "\n",
    "    def generate_synthetic_data(self, data: pd.DataFrame, num_samples: int = 1000, epochs: int = 100, chunk_size: int = 1000) -> pd.DataFrame:\n",
    "        \"\"\"Generate synthetic data using CTGAN\"\"\"\n",
    "        self.logger.info(\"Starting synthetic data generation\")\n",
    "        try:\n",
    "            # Only process selected categorical columns\n",
    "            processed_data = self.preprocess_data(data)\n",
    "            \n",
    "            # Initialize CTGAN with conservative parameters\n",
    "            synthesizer = CTGAN(\n",
    "                epochs=epochs,\n",
    "                batch_size=500,\n",
    "                generator_dim=(128, 128),\n",
    "                discriminator_dim=(128, 128),\n",
    "                embedding_dim=128,\n",
    "                verbose=True\n",
    "            )\n",
    "\n",
    "            # Fit the model with all columns as discrete\n",
    "            self.logger.info(\"Training CTGAN model...\")\n",
    "            synthesizer.fit(processed_data, discrete_columns=self.categorical_columns)\n",
    "\n",
    "            # Generate synthetic data\n",
    "            self.logger.info(f\"Generating {num_samples} synthetic samples...\")\n",
    "            synthetic_data = synthesizer.sample(num_samples)\n",
    "\n",
    "            # Post-process to ensure string type and handle missing values\n",
    "            for col in self.categorical_columns:\n",
    "                synthetic_data[col] = synthetic_data[col].astype(str)\n",
    "                synthetic_data[col] = synthetic_data[col].replace('MISSING', np.nan)\n",
    "\n",
    "            self.logger.info(f\"Generated {len(synthetic_data)} synthetic samples\")\n",
    "            return synthetic_data\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in generate_synthetic_data: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def validate_synthetic_data(self, real_data: pd.DataFrame, synthetic_data: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Validate synthetic data against real data for specified categorical columns.\"\"\"\n",
    "        self.logger.info(\"Validating synthetic data\")\n",
    "        \n",
    "        metrics = {\n",
    "            \"real_shape\": real_data.shape,\n",
    "            \"synthetic_shape\": synthetic_data.shape,\n",
    "            \"column_match\": all(col in synthetic_data.columns for col in real_data.columns),\n",
    "            \"basic_stats\": {}\n",
    "        }\n",
    "        \n",
    "        for col in self.categorical_columns:\n",
    "            if col in real_data.columns and col in synthetic_data.columns:\n",
    "                metrics[\"basic_stats\"][col] = {\n",
    "                    \"unique_values_real\": real_data[col].nunique(),\n",
    "                    \"unique_values_synthetic\": synthetic_data[col].nunique()\n",
    "                }\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "    def save_outputs(self, synthetic_data: pd.DataFrame, validation_metrics: Dict) -> None:\n",
    "        \"\"\"Save synthetic data and metadata.\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Filter only the user-selected categorical columns\n",
    "        output_data = synthetic_data[self.categorical_columns].copy()\n",
    "        self.logger.info(f\"Saving {len(self.categorical_columns)} categorical columns: {', '.join(self.categorical_columns)}\")\n",
    "        \n",
    "        # Save filtered data\n",
    "        output_file = os.path.join(self.output_dir, f\"synthetic_data_{timestamp}.csv\")\n",
    "        output_data.to_csv(output_file, index=False)\n",
    "        \n",
    "        metadata = {\n",
    "            \"generation_timestamp\": timestamp,\n",
    "            \"original_file\": self.input_file,\n",
    "            \"num_samples\": len(output_data),\n",
    "            \"categorical_columns\": self.categorical_columns,\n",
    "            \"validation_metrics\": validation_metrics,\n",
    "            **self.metadata\n",
    "        }\n",
    "        \n",
    "        metadata_file = os.path.join(self.output_dir, f\"metadata_{timestamp}.json\")\n",
    "        with open(metadata_file, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        self.logger.info(f\"Saved synthetic data to {output_file}\")\n",
    "        self.logger.info(f\"Saved metadata to {metadata_file}\")\n",
    "\n",
    "    def create_metadata_file(self, synthetic_data: pd.DataFrame, timestamp: str) -> None:\n",
    "        \"\"\"Create a metadata.json file with dataset details.\"\"\"\n",
    "        metadata = {\n",
    "            \"name\": \"Synthetic Healthcare Dataset\",\n",
    "            \"description\": \"A privacy-preserving synthetic dataset for healthcare analysis.\",\n",
    "            \"columns\": [\n",
    "                {\"name\": col, \"type\": \"categorical\"} for col in self.categorical_columns\n",
    "            ],\n",
    "            \"size\": f\"{len(synthetic_data)} rows\"\n",
    "        }\n",
    "        metadata_file = os.path.join(self.output_dir, f\"metadata_{timestamp}.json\")\n",
    "        with open(metadata_file, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        self.logger.info(f\"Saved metadata to {metadata_file}\")\n",
    "\n",
    "    def compress_files(self, timestamp: str) -> None:\n",
    "        \"\"\"Compress the dataset and documentation into a zip file.\"\"\"\n",
    "        zip_filename = os.path.join(self.output_dir, f\"dataset_package_{timestamp}.zip\")\n",
    "        shutil.make_archive(zip_filename.replace('.zip', ''), 'zip', self.output_dir)\n",
    "        self.logger.info(f\"Compressed files into {zip_filename}\")\n",
    "\n",
    "    def run_pipeline(self, num_samples: int = 1000, chunk_size: int = 10000, epochs: int = 100, **kwargs) -> None:\n",
    "        \"\"\"Run the synthetic data generation pipeline.\"\"\"\n",
    "        try:\n",
    "            real_data = self.load_data()\n",
    "            synthetic_data = self.generate_synthetic_data(real_data, num_samples=num_samples, epochs=epochs, chunk_size=chunk_size, **kwargs)\n",
    "            validation_metrics = self.validate_synthetic_data(real_data, synthetic_data)\n",
    "            self.save_outputs(synthetic_data, validation_metrics)\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            self.create_metadata_file(synthetic_data, timestamp)\n",
    "            self.compress_files(timestamp)\n",
    "            self.logger.info(\"Pipeline completed successfully\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Pipeline failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "# Example usage\n",
    "pipeline = SyntheticDataPipeline(\n",
    "    input_file='path/to/your/input.csv',\n",
    "    categorical_columns=['column1', 'column2'],\n",
    "    output_dir='output'\n",
    ")\n",
    "pipeline.run_pipeline(num_samples=1000, epochs=100, chunk_size=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block 4: Package the Dataset\n",
    "**Objective:** Prepare the dataset for distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a README File\n",
    "readme_content = '''\n",
    "# Synthetic Healthcare Dataset\n",
    "\n",
    "## Description\n",
    "A privacy-preserving synthetic dataset for healthcare analysis.\n",
    "\n",
    "## Instructions for Use\n",
    "1. Load the dataset using your preferred data analysis tool.\n",
    "2. Use the metadata.json file to understand the structure and types of columns.\n",
    "\n",
    "## Licensing\n",
    "This dataset is licensed under the MIT License.\n",
    "'''\n",
    "\n",
    "with open(os.path.join(pipeline.output_dir, 'README.md'), 'w') as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "# Add Metadata\n",
    "metadata = {\n",
    "    \"name\": \"Synthetic Healthcare Dataset\",\n",
    "    \"description\": \"A privacy-preserving synthetic dataset for healthcare analysis.\",\n",
    "    \"columns\": [\n",
    "        {\"name\": \"Age\", \"type\": \"integer\", \"range\": \"0-100\"},\n",
    "        {\"name\": \"Diagnosis\", \"type\": \"categorical\", \"values\": [\"Diabetes\", \"Hypertension\"]}\n",
    "    ],\n",
    "    \"size\": \"100,000 rows\"\n",
    "}\n",
    "\n",
    "with open(os.path.join(pipeline.output_dir, 'metadata.json'), 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "# Compress Files\n",
    "shutil.make_archive('dataset_package', 'zip', pipeline.output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block 5: Monetize the Dataset\n",
    "**Objective:** Choose platforms and methods to sell your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Upload to Platforms:**\n",
    "   - **Hugging Face:**\n",
    "     - Create a repository for your dataset.\n",
    "     - Upload a free sample and lock full access behind a paid plan.\n",
    "   - **Kaggle:**\n",
    "     - Showcase the dataset with clear explanations and use cases.\n",
    "     - Include links for purchase or further access.\n",
    "\n",
    "2. **Create Pricing Plans:**\n",
    "   - Define subscription tiers for updates or different dataset sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block 6: Market the Dataset\n",
    "**Objective:** Increase visibility and attract buyers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Build a Portfolio:**\n",
    "   - Create a GitHub repository or personal website showcasing the dataset and its applications.\n",
    "\n",
    "2. **Promote on Social Media:**\n",
    "   - Share posts with examples and visuals on LinkedIn, Twitter, Reddit, and relevant forums.\n",
    "\n",
    "3. **Write Blogs/Tutorials:**\n",
    "   - Publish step-by-step guides on using your dataset for AI/ML tasks."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
